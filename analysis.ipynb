{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad684cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, balanced_accuracy_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style untuk plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff66058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi helper untuk basic info\n",
    "def basic_info(df, filename):\n",
    "    print(f\"\\nüìä {filename}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Memory usage: {df.memory_usage().sum() / 1024:.2f} KB\")\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d720c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "file_names = {\n",
    "    'absensi': 'MineToday Dataset/train/train_absensi.csv',\n",
    "    'mini_project': 'MineToday Dataset/train/train_mini_project.csv', \n",
    "    'pendaftaran': 'MineToday Dataset/train/train_pendaftaran.csv',\n",
    "    'pretest_ml': 'MineToday Dataset/train/train_pretest_ml.csv',\n",
    "    'pretest_py': 'MineToday Dataset/train/train_pretest_py.csv',\n",
    "    'pretest_st': 'MineToday Dataset/train/train_pretest_st.csv',\n",
    "    'weekly_quiz': 'MineToday Dataset/train/train_weekly_quiz.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62d18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    datasets = {}\n",
    "    for name, path in file_names.items():\n",
    "        try:\n",
    "            datasets[name] = pd.read_csv(path)\n",
    "            print(f\"‚úÖ {name}: {datasets[name].shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå {name}: File not found at {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {name}: Error loading - {e}\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e16e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis detail setiap dataset\n",
    "for file_name, df in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    sample_data = basic_info(df, file_name)\n",
    "    print(f\"\\nSample data (first 3 rows):\")\n",
    "    print(sample_data.head(3))\n",
    "    \n",
    "    # Cek unique values untuk kolom kategorikal\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüè∑Ô∏è  Categorical columns unique values:\")\n",
    "        for col in categorical_cols[:5]:  # Limit to first 5 columns\n",
    "            unique_vals = df[col].nunique()\n",
    "            print(f\"  {col}: {unique_vals} unique values\")\n",
    "            if unique_vals <= 10:\n",
    "                print(f\"    Values: {df[col].unique()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c1f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîó PARTICIPANT ID ANALYSIS\n",
      "----------------------------------------\n",
      "Potential ID columns:\n"
     ]
    }
   ],
   "source": [
    "# Analisis ID peserta untuk join datasets\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üîó PARTICIPANT ID ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Cari kolom yang mungkin berisi ID peserta\n",
    "id_candidates = []\n",
    "for file_name, df in datasets.items():\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.lower() for keyword in ['id', 'email', 'nama', 'timestamp']):\n",
    "            id_candidates.append((file_name, col, df[col].nunique()))\n",
    "\n",
    "print(\"Potential ID columns:\")\n",
    "for file_name, col, unique_count in id_candidates:\n",
    "    print(f\"  {file_name}: {col} ({unique_count} unique values)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc32789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ TIMESTAMP ANALYSIS\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cek timestamp patterns\n",
    "print(f\"\\n‚è∞ TIMESTAMP ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "for file_name, df in datasets.items():\n",
    "    timestamp_cols = [col for col in df.columns if 'timestamp' in col.lower() or 'tanggal' in col.lower()]\n",
    "    if timestamp_cols:\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        for col in timestamp_cols:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "            print(f\"    Sample: {df[col].dropna().head(2).tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a598903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä NUMERICAL COLUMNS SUMMARY\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics untuk numerical columns\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä NUMERICAL COLUMNS SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for file_name, df in datasets.items():\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        print(df[numerical_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6027d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ POTENTIAL TARGET PATTERNS\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pattern analysis untuk potential target creation\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ POTENTIAL TARGET PATTERNS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analisis completion patterns\n",
    "completion_indicators = []\n",
    "for file_name, df in datasets.items():\n",
    "    if 'absensi' in file_name:\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        if 'Pertemuan ke' in df.columns or 'Pertemuan ke-' in df.columns:\n",
    "            pertemuan_col = 'Pertemuan ke' if 'Pertemuan ke' in df.columns else 'Pertemuan ke-'\n",
    "            print(f\"  Column: {pertemuan_col}\")\n",
    "            print(f\"  Data type: {df[pertemuan_col].dtype}\")\n",
    "            print(f\"  Unique values: {sorted(df[pertemuan_col].dropna().unique())}\")\n",
    "            print(f\"  Missing values: {df[pertemuan_col].isnull().sum()}\")\n",
    "            \n",
    "            # Convert to numeric untuk cari max\n",
    "            try:\n",
    "                numeric_values = pd.to_numeric(df[pertemuan_col], errors='coerce')\n",
    "                max_pertemuan = numeric_values.max()\n",
    "                print(f\"  Max pertemuan: {max_pertemuan}\")\n",
    "                print(f\"  Min pertemuan: {numeric_values.min()}\")\n",
    "                print(f\"  Total attendances: {len(df)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing pertemuan: {e}\")\n",
    "            \n",
    "    elif 'quiz' in file_name:\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        print(f\"  Total quiz records: {df.shape[0]}\")\n",
    "        # Cek jika ada kolom score/nilai\n",
    "        score_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['score', 'nilai', 'point'])]\n",
    "        if score_cols:\n",
    "            for col in score_cols[:3]:  # Limit to first 3 score columns\n",
    "                print(f\"  {col}: mean={df[col].mean():.2f}, std={df[col].std():.2f}\")\n",
    "        \n",
    "    elif 'mini_project' in file_name:\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        print(f\"  Total project submissions: {df.shape[0]}\")\n",
    "        # Cek jika ada link submissions\n",
    "        link_cols = [col for col in df.columns if 'link' in col.lower() or 'url' in col.lower()]\n",
    "        if link_cols:\n",
    "            for col in link_cols:\n",
    "                non_empty = df[col].dropna().shape[0]\n",
    "                print(f\"  {col}: {non_empty} non-empty submissions\")\n",
    "        \n",
    "    elif 'pretest' in file_name:\n",
    "        test_type = file_name.split('_')[-1].replace('.csv', '').upper()\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        print(f\"  Pretest {test_type} participants: {df.shape[0]}\")\n",
    "        # Cek score columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            for col in numeric_cols[:3]:  # First 3 numeric columns\n",
    "                print(f\"  {col}: mean={df[col].mean():.2f}, range=[{df[col].min():.1f}, {df[col].max():.1f}]\")\n",
    "    \n",
    "    elif 'pendaftaran' in file_name:\n",
    "        print(f\"\\n{file_name}:\")\n",
    "        print(f\"  Total registrations: {df.shape[0]}\")\n",
    "        if 'Status' in df.columns:\n",
    "            status_counts = df['Status'].value_counts()\n",
    "            print(f\"  Status distribution: {dict(status_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b92e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà CROSS-DATASET ANALYSIS\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìà CROSS-DATASET ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Coba identifikasi common participants\n",
    "participant_counts = {}\n",
    "for file_name, df in datasets.items():\n",
    "    # Cari kolom yang mungkin identifier\n",
    "    for col in df.columns:\n",
    "        if 'email' in col.lower() or 'nama' in col.lower():\n",
    "            unique_participants = df[col].nunique()\n",
    "            participant_counts[f\"{file_name}_{col}\"] = unique_participants\n",
    "            print(f\"{file_name} - {col}: {unique_participants} unique participants\")\n",
    "\n",
    "if participant_counts:\n",
    "    print(f\"\\nParticipant overlap analysis needed for joining datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a506be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ DATA INTEGRATION\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ DATA INTEGRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def parse_score(score_str):\n",
    "    \"\"\"Convert '80 / 100' to 0.8\"\"\"\n",
    "    if pd.isna(score_str):\n",
    "        return np.nan\n",
    "    if isinstance(score_str, str) and '/' in score_str:\n",
    "        try:\n",
    "            num, den = score_str.split('/')\n",
    "            return float(num.strip()) / float(den.strip())\n",
    "        except:\n",
    "            return np.nan\n",
    "    return score_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1b2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meeting_number(meeting_str):\n",
    "    \"\"\"Extract number from 'Pertemuan 12'\"\"\"\n",
    "    if pd.isna(meeting_str):\n",
    "        return np.nan\n",
    "    if isinstance(meeting_str, str):\n",
    "        match = re.search(r'\\d+', meeting_str)\n",
    "        return int(match.group()) if match else np.nan\n",
    "    return meeting_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a449af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è FEATURE ENGINEERING\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ†Ô∏è FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d884f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attendance_features(df_absensi):\n",
    "    print(\"\\nüìä Processing Attendance Data...\")\n",
    "    \n",
    "    # Parse meeting numbers dari 'Pertemuan ke' column\n",
    "    df_absensi['meeting_num'] = df_absensi['Pertemuan ke'].apply(extract_meeting_number)\n",
    "    \n",
    "    # Aggregate per participant\n",
    "    attendance_stats = df_absensi.groupby('id').agg({\n",
    "        'meeting_num': ['count', 'max', 'min'],\n",
    "        'Kualitas materi ': 'mean',\n",
    "        'Kualitas trainer ': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    attendance_stats.columns = [\n",
    "        'total_meetings_attended', 'highest_meeting', 'first_meeting',\n",
    "        'avg_material_quality', 'avg_trainer_quality'\n",
    "    ]\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    MAX_MEETINGS = 30  # Assuming 30 total meetings\n",
    "    attendance_stats['attendance_rate'] = (\n",
    "        attendance_stats['total_meetings_attended'] / MAX_MEETINGS\n",
    "    ).clip(0, 1).round(3)\n",
    "    \n",
    "    # Engagement score (high attendance + good ratings)\n",
    "    attendance_stats['engagement_score'] = (\n",
    "        attendance_stats['attendance_rate'] * 0.6 + \n",
    "        (attendance_stats['avg_material_quality'] / 5) * 0.2 +\n",
    "        (attendance_stats['avg_trainer_quality'] / 5) * 0.2\n",
    "    ).round(3)\n",
    "    \n",
    "    return attendance_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb3eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assessment_features(datasets):\n",
    "    print(\"\\nüìä Processing Assessment Scores...\")\n",
    "    \n",
    "    assessment_features = []\n",
    "    \n",
    "    # Process each pretest\n",
    "    for test_name in ['pretest_ml', 'pretest_py', 'pretest_st']:\n",
    "        df = datasets[test_name].copy()\n",
    "        \n",
    "        # Parse scores\n",
    "        df['score_normalized'] = df['Score'].apply(parse_score)\n",
    "        \n",
    "        # Get best attempt per participant\n",
    "        best_scores = df.groupby('id')['score_normalized'].max().reset_index()\n",
    "        best_scores.columns = ['id', f'{test_name}_best_score']\n",
    "        \n",
    "        assessment_features.append(best_scores)\n",
    "    \n",
    "    # Merge all assessment scores\n",
    "    combined_assessments = assessment_features[0]\n",
    "    for df in assessment_features[1:]:\n",
    "        combined_assessments = combined_assessments.merge(df, on='id', how='outer')\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    score_cols = ['pretest_ml_best_score', 'pretest_py_best_score', 'pretest_st_best_score']\n",
    "    \n",
    "    combined_assessments['avg_pretest_score'] = (\n",
    "        combined_assessments[score_cols].mean(axis=1, skipna=True).round(3)\n",
    "    )\n",
    "    \n",
    "    combined_assessments['assessment_completion_rate'] = (\n",
    "        combined_assessments[score_cols].notna().sum(axis=1) / len(score_cols)\n",
    "    ).round(3)\n",
    "    \n",
    "    # Performance category\n",
    "    combined_assessments['performance_level'] = pd.cut(\n",
    "        combined_assessments['avg_pretest_score'], \n",
    "        bins=[0, 0.5, 0.7, 0.85, 1.0], \n",
    "        labels=['Low', 'Medium', 'High', 'Excellent']\n",
    "    )\n",
    "    \n",
    "    return combined_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e61143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_features(datasets):\n",
    "    print(\"\\nüìä Processing Submission Data...\")\n",
    "    \n",
    "    submission_data = []\n",
    "    \n",
    "    # Mini project submissions\n",
    "    mini_project = datasets['mini_project'].groupby('id').size().reset_index()\n",
    "    mini_project.columns = ['id', 'mini_project_count']\n",
    "    mini_project['has_mini_project'] = (mini_project['mini_project_count'] > 0).astype(int)\n",
    "    submission_data.append(mini_project[['id', 'has_mini_project']])\n",
    "    \n",
    "    # Weekly quiz submissions\n",
    "    weekly_quiz = datasets['weekly_quiz'].groupby('id').size().reset_index()\n",
    "    weekly_quiz.columns = ['id', 'quiz_submissions']\n",
    "    weekly_quiz['has_weekly_quiz'] = (weekly_quiz['quiz_submissions'] > 0).astype(int)\n",
    "    submission_data.append(weekly_quiz[['id', 'has_weekly_quiz']])\n",
    "    \n",
    "    # Combine submission features\n",
    "    combined_submissions = submission_data[0]\n",
    "    for df in submission_data[1:]:\n",
    "        combined_submissions = combined_submissions.merge(df, on='id', how='outer')\n",
    "    \n",
    "    # Fill missing values\n",
    "    combined_submissions = combined_submissions.fillna(0)\n",
    "    \n",
    "    # Overall submission rate\n",
    "    combined_submissions['total_submissions'] = (\n",
    "        combined_submissions['has_mini_project'] + \n",
    "        combined_submissions['has_weekly_quiz']\n",
    "    )\n",
    "    \n",
    "    return combined_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09f8f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_registration_features(df_registration):\n",
    "    print(\"\\nüìä Processing Registration Data...\")\n",
    "    \n",
    "    reg_features = df_registration[['id', 'Status', 'Pilihan Jadwal Kelas']].copy()\n",
    "    \n",
    "    # Encode status\n",
    "    status_mapping = {\n",
    "        'Mahasiswa': 1, 'Fresh Graduates': 2, \n",
    "        'Pekerja aktif': 3, 'Umum': 4\n",
    "    }\n",
    "    reg_features['status_encoded'] = reg_features['Status'].map(status_mapping)\n",
    "    \n",
    "    # Extract batch info\n",
    "    reg_features['batch'] = reg_features['Pilihan Jadwal Kelas'].str.extract(r'Batch (\\d+)')\n",
    "    reg_features['batch'] = pd.to_numeric(reg_features['batch'], errors='coerce')\n",
    "    \n",
    "    return reg_features[['id', 'status_encoded', 'batch']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5c8bacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ absensi: (11714, 12)\n",
      "‚úÖ mini_project: (468, 5)\n",
      "‚úÖ pendaftaran: (492, 9)\n",
      "‚úÖ pretest_ml: (502, 14)\n",
      "‚úÖ pretest_py: (544, 14)\n",
      "‚úÖ pretest_st: (500, 19)\n",
      "‚úÖ weekly_quiz: (487, 5)\n",
      "\n",
      "üìã Unique participants per dataset:\n",
      "  absensi: 509 unique IDs\n",
      "  mini_project: 468 unique IDs\n",
      "  pendaftaran: 492 unique IDs\n",
      "  pretest_ml: 494 unique IDs\n",
      "  pretest_py: 526 unique IDs\n",
      "  pretest_st: 497 unique IDs\n",
      "  weekly_quiz: 483 unique IDs\n",
      "\n",
      "üë• Total unique participants across all datasets: 549\n",
      "‚úÖ Master participant table created with 549 participants\n",
      "\n",
      "üìä Processing Attendance Data...\n",
      "\n",
      "üìä Processing Assessment Scores...\n",
      "\n",
      "üìä Processing Submission Data...\n",
      "\n",
      "üìä Processing Registration Data...\n"
     ]
    }
   ],
   "source": [
    "# Execute feature engineering\n",
    "datasets = load_datasets()\n",
    "\n",
    "# Get all unique participant IDs (missing from script 1)\n",
    "print(f\"\\nüìã Unique participants per dataset:\")\n",
    "all_participant_ids = set()\n",
    "for name, df in datasets.items():\n",
    "    unique_ids = df['id'].nunique()\n",
    "    print(f\"  {name}: {unique_ids} unique IDs\")\n",
    "    all_participant_ids.update(df['id'].unique())\n",
    "\n",
    "print(f\"\\nüë• Total unique participants across all datasets: {len(all_participant_ids)}\")\n",
    "\n",
    "# Create master participant table\n",
    "master_df = pd.DataFrame({'id': list(all_participant_ids)})\n",
    "print(f\"‚úÖ Master participant table created with {len(master_df)} participants\")\n",
    "\n",
    "attendance_features = create_attendance_features(datasets['absensi'])\n",
    "assessment_features = create_assessment_features(datasets)\n",
    "submission_features = create_submission_features(datasets)\n",
    "registration_features = create_registration_features(datasets['pendaftaran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e37ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Merging all features...\n",
      "  ‚úÖ Merged attendance: 549 ‚Üí 549 rows\n",
      "  ‚úÖ Merged assessment: 549 ‚Üí 549 rows\n",
      "  ‚úÖ Merged submission: 549 ‚Üí 549 rows\n",
      "  ‚úÖ Merged registration: 549 ‚Üí 549 rows\n",
      "\n",
      "üéØ Final feature matrix: (549, 19)\n",
      "Columns: ['id', 'total_meetings_attended', 'highest_meeting', 'first_meeting', 'avg_material_quality', 'avg_trainer_quality', 'attendance_rate', 'engagement_score', 'pretest_ml_best_score', 'pretest_py_best_score', 'pretest_st_best_score', 'avg_pretest_score', 'assessment_completion_rate', 'performance_level', 'has_mini_project', 'has_weekly_quiz', 'total_submissions', 'status_encoded', 'batch']\n",
      "\n",
      "üìä Sample of engineered features:\n",
      "                                     id  total_meetings_attended  \\\n",
      "0  366c8261-243e-4eab-9695-6df81979597a                     18.0   \n",
      "1  84d29b44-26c6-4b05-8a67-31831d35be32                     28.0   \n",
      "2  e0766991-32fc-4294-b1e2-a0dff52c64f2                     23.0   \n",
      "3  a59fa924-4733-471c-8754-f6595cd0e6f1                     27.0   \n",
      "4  ab9d4c1f-1773-4f12-88e1-ecd78f51e309                     30.0   \n",
      "\n",
      "   highest_meeting  first_meeting  avg_material_quality  avg_trainer_quality  \\\n",
      "0             18.0            1.0                 4.500                4.611   \n",
      "1             28.0            1.0                 4.357                4.536   \n",
      "2             23.0            1.0                 4.522                4.435   \n",
      "3             27.0            1.0                 4.333                4.444   \n",
      "4             30.0            1.0                 4.567                4.433   \n",
      "\n",
      "   attendance_rate  engagement_score  pretest_ml_best_score  \\\n",
      "0            0.600             0.724                    0.5   \n",
      "1            0.933             0.916                    0.8   \n",
      "2            0.767             0.818                    0.5   \n",
      "3            0.900             0.891                    0.5   \n",
      "4            1.000             0.960                    0.8   \n",
      "\n",
      "   pretest_py_best_score  pretest_st_best_score  avg_pretest_score  \\\n",
      "0                    1.0                   0.90              0.800   \n",
      "1                    0.7                   0.85              0.783   \n",
      "2                    0.9                   0.90              0.767   \n",
      "3                    0.7                   0.85              0.683   \n",
      "4                    0.8                   1.00              0.867   \n",
      "\n",
      "   assessment_completion_rate performance_level  has_mini_project  \\\n",
      "0                         1.0              High               1.0   \n",
      "1                         1.0              High               1.0   \n",
      "2                         1.0              High               1.0   \n",
      "3                         1.0            Medium               1.0   \n",
      "4                         1.0         Excellent               1.0   \n",
      "\n",
      "   has_weekly_quiz  total_submissions  status_encoded  batch  \n",
      "0              1.0                2.0             3.0    NaN  \n",
      "1              1.0                2.0             2.0    9.0  \n",
      "2              1.0                2.0             2.0    9.0  \n",
      "3              1.0                2.0             1.0    8.0  \n",
      "4              1.0                2.0             2.0    9.0  \n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüîÑ Merging all features...\")\n",
    "final_features = master_df.copy()\n",
    "\n",
    "# Merge each feature set\n",
    "feature_sets = [\n",
    "    ('attendance', attendance_features),\n",
    "    ('assessment', assessment_features), \n",
    "    ('submission', submission_features),\n",
    "    ('registration', registration_features)\n",
    "]\n",
    "\n",
    "for name, features in feature_sets:\n",
    "    before_count = len(final_features)\n",
    "    final_features = final_features.merge(features, on='id', how='left')\n",
    "    after_count = len(final_features)\n",
    "    print(f\"  ‚úÖ Merged {name}: {before_count} ‚Üí {after_count} rows\")\n",
    "\n",
    "print(f\"\\nüéØ Final feature matrix: {final_features.shape}\")\n",
    "print(f\"Columns: {list(final_features.columns)}\")\n",
    "\n",
    "# Show sample of final features\n",
    "print(f\"\\nüìä Sample of engineered features:\")\n",
    "print(final_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28c8455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TARGET CREATION\n",
      "==================================================\n",
      "\n",
      "üîß Preprocessing for target creation...\n",
      "‚úÖ Target features prepared for 549 participants\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ TARGET CREATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fill missing values untuk target creation\n",
    "print(\"\\nüîß Preprocessing for target creation...\")\n",
    "\n",
    "# Select features for target creation\n",
    "target_features = [\n",
    "    'attendance_rate', 'highest_meeting', 'total_meetings_attended',\n",
    "    'avg_pretest_score', 'assessment_completion_rate', \n",
    "    'total_submissions', 'engagement_score'\n",
    "]\n",
    "\n",
    "# Create a subset with target features\n",
    "target_df = final_features[['id'] + target_features].copy()\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "target_df[target_features] = imputer.fit_transform(target_df[target_features])\n",
    "\n",
    "print(f\"‚úÖ Target features prepared for {len(target_df)} participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa89c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD 1: RULE-BASED LABELING\n",
    "# ============================================================================\n",
    "\n",
    "def create_rule_based_labels(df):\n",
    "    print(\"\\nüìã Method 1: Rule-Based Labeling\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Define graduation criteria\n",
    "    conditions = {\n",
    "        'high_attendance': df['attendance_rate'] >= 0.75,  # Attended 75%+ meetings\n",
    "        'reached_advanced': df['highest_meeting'] >= 20,   # Reached meeting 20+\n",
    "        'good_performance': df['avg_pretest_score'] >= 0.6, # Average score 60%+\n",
    "        'active_submission': df['total_submissions'] >= 1,  # Submitted at least 1 project\n",
    "        'high_engagement': df['engagement_score'] >= 0.7   # Overall engagement 70%+\n",
    "    }\n",
    "    \n",
    "    # Calculate completion score\n",
    "    df['completion_score'] = (\n",
    "        conditions['high_attendance'].astype(int) * 0.35 +      # Attendance weight\n",
    "        conditions['reached_advanced'].astype(int) * 0.25 +     # Progress weight  \n",
    "        conditions['good_performance'].astype(int) * 0.2 +      # Performance weight\n",
    "        conditions['active_submission'].astype(int) * 0.1 +     # Submission weight\n",
    "        conditions['high_engagement'].astype(int) * 0.1         # Engagement weight\n",
    "    )\n",
    "    \n",
    "    # Create binary labels\n",
    "    GRADUATION_THRESHOLD = 0.6  # Need 60% overall score to graduate\n",
    "    df['graduated_rule'] = (df['completion_score'] >= GRADUATION_THRESHOLD).astype(int)\n",
    "    \n",
    "    # Show criteria breakdown\n",
    "    print(\"Graduation Criteria:\")\n",
    "    for criterion, condition in conditions.items():\n",
    "        count = condition.sum()\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {criterion}: {count} participants ({pct:.1f}%)\")\n",
    "    \n",
    "    graduate_count = df['graduated_rule'].sum()\n",
    "    print(f\"\\nüéì Rule-based graduation rate: {graduate_count}/{len(df)} ({graduate_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1306f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD 2: CLUSTERING-BASED LABELING  \n",
    "# ============================================================================\n",
    "\n",
    "def create_cluster_based_labels(df):\n",
    "    print(\"\\nüîç Method 2: Clustering-Based Labeling\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    cluster_features = [\n",
    "        'attendance_rate', 'avg_pretest_score', 'total_submissions', \n",
    "        'engagement_score', 'highest_meeting'\n",
    "    ]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df[cluster_features])\n",
    "    \n",
    "    # Apply K-means clustering (k=3: low, medium, high performers)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Analyze clusters to identify high performers\n",
    "    cluster_analysis = df.groupby('cluster')[cluster_features].mean().round(3)\n",
    "    print(\"Cluster Analysis:\")\n",
    "    print(cluster_analysis)\n",
    "    \n",
    "    # Identify the \"high performer\" cluster (highest attendance + performance)\n",
    "    cluster_scores = cluster_analysis['attendance_rate'] + cluster_analysis['avg_pretest_score']\n",
    "    high_performer_cluster = cluster_scores.idxmax()\n",
    "    \n",
    "    # Create binary labels (high performer cluster = graduated)\n",
    "    df['graduated_cluster'] = (df['cluster'] == high_performer_cluster).astype(int)\n",
    "    \n",
    "    cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        status = \"HIGH PERFORMER\" if cluster == high_performer_cluster else \"REGULAR\"\n",
    "        print(f\"  Cluster {cluster}: {count} participants ({pct:.1f}%) - {status}\")\n",
    "    \n",
    "    graduate_count = df['graduated_cluster'].sum()\n",
    "    print(f\"\\nüéì Cluster-based graduation rate: {graduate_count}/{len(df)} ({graduate_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d36c3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Method 1: Rule-Based Labeling\n",
      "----------------------------------------\n",
      "Graduation Criteria:\n",
      "  high_attendance: 320 participants (58.3%)\n",
      "  reached_advanced: 465 participants (84.7%)\n",
      "  good_performance: 527 participants (96.0%)\n",
      "  active_submission: 549 participants (100.0%)\n",
      "  high_engagement: 518 participants (94.4%)\n",
      "\n",
      "üéì Rule-based graduation rate: 436/549 (79.4%)\n",
      "\n",
      "üîç Method 2: Clustering-Based Labeling\n",
      "----------------------------------------\n",
      "Cluster Analysis:\n",
      "         attendance_rate  avg_pretest_score  total_submissions  \\\n",
      "cluster                                                          \n",
      "0                  0.909              0.725              1.945   \n",
      "1                  0.698              0.781              1.975   \n",
      "2                  0.000              0.784              1.828   \n",
      "\n",
      "         engagement_score  highest_meeting  \n",
      "cluster                                     \n",
      "0                   0.906           27.326  \n",
      "1                   0.779           21.042  \n",
      "2                   0.368           24.000  \n",
      "\n",
      "Cluster distribution:\n",
      "  Cluster 0: 236 participants (43.0%) - HIGH PERFORMER\n",
      "  Cluster 1: 284 participants (51.7%) - REGULAR\n",
      "  Cluster 2: 29 participants (5.3%) - REGULAR\n",
      "\n",
      "üéì Cluster-based graduation rate: 236/549 (43.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE TARGET CREATION\n",
    "# ============================================================================\n",
    "\n",
    "# Create labels using both methods\n",
    "target_df = create_rule_based_labels(target_df)\n",
    "target_df = create_cluster_based_labels(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "053f4c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç LABEL VALIDATION\n",
      "==================================================\n",
      "Agreement between methods: 349/549 (63.6%)\n",
      "\n",
      "Cross-tabulation:\n",
      "graduated_cluster    0    1  All\n",
      "graduated_rule                  \n",
      "0                  113    0  113\n",
      "1                  200  236  436\n",
      "All                313  236  549\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LABEL VALIDATION & COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç LABEL VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare the two labeling methods\n",
    "agreement = (target_df['graduated_rule'] == target_df['graduated_cluster']).sum()\n",
    "agreement_rate = agreement / len(target_df) * 100\n",
    "\n",
    "print(f\"Agreement between methods: {agreement}/{len(target_df)} ({agreement_rate:.1f}%)\")\n",
    "\n",
    "# Cross-tabulation\n",
    "crosstab = pd.crosstab(\n",
    "    target_df['graduated_rule'], \n",
    "    target_df['graduated_cluster'], \n",
    "    margins=True\n",
    ")\n",
    "print(f\"\\nCross-tabulation:\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "725d9a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ FINAL TARGET SELECTION\n",
      "------------------------------\n",
      "Final graduation rate: 436/549 (79.4%)\n",
      "\n",
      "Final Target Distribution:\n",
      "  Graduated (1): 436 participants\n",
      "  Not Graduated (0): 113 participants\n",
      "‚ö†Ô∏è  Class imbalance detected! Consider SMOTE for modeling.\n",
      "\n",
      "üéâ FINAL DATASET READY!\n",
      "Shape: (549, 21)\n",
      "Features: 20\n",
      "Target column: 'graduated_final'\n",
      "\n",
      "üìä Final Dataset Sample:\n",
      "                                     id  attendance_rate  avg_pretest_score  \\\n",
      "0  366c8261-243e-4eab-9695-6df81979597a            0.600              0.800   \n",
      "1  84d29b44-26c6-4b05-8a67-31831d35be32            0.933              0.783   \n",
      "2  e0766991-32fc-4294-b1e2-a0dff52c64f2            0.767              0.767   \n",
      "3  a59fa924-4733-471c-8754-f6595cd0e6f1            0.900              0.683   \n",
      "4  ab9d4c1f-1773-4f12-88e1-ecd78f51e309            1.000              0.867   \n",
      "5  f29b1b86-ab96-4876-889d-70b5955a01c7            0.900              0.600   \n",
      "6  e62e4b0c-e929-48f4-ad21-f172df57e747            0.767              0.833   \n",
      "7  4073acaa-d216-4e04-8729-bfadada4d095            0.667              0.617   \n",
      "8  28c21980-3720-4511-bec7-cc2e6275750a            0.933              0.767   \n",
      "9  6f2f152e-fe85-4611-857a-528266761927            0.667              0.750   \n",
      "\n",
      "   total_submissions  graduated_final  \n",
      "0                2.0                0  \n",
      "1                2.0                1  \n",
      "2                2.0                1  \n",
      "3                2.0                1  \n",
      "4                2.0                1  \n",
      "5                2.0                1  \n",
      "6                2.0                1  \n",
      "7                2.0                1  \n",
      "8                2.0                1  \n",
      "9                2.0                1  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL TARGET SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüéØ FINAL TARGET SELECTION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Use ensemble approach: agree on both methods OR high completion score\n",
    "target_df['graduated_final'] = (\n",
    "    (target_df['graduated_rule'] == 1) & \n",
    "    (target_df['graduated_cluster'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# For cases where methods disagree, use completion score as tiebreaker\n",
    "disagreement_mask = target_df['graduated_rule'] != target_df['graduated_cluster']\n",
    "high_score_mask = target_df['completion_score'] >= 0.65\n",
    "\n",
    "target_df.loc[disagreement_mask & high_score_mask, 'graduated_final'] = 1\n",
    "\n",
    "final_graduate_count = target_df['graduated_final'].sum()\n",
    "final_rate = final_graduate_count / len(target_df) * 100\n",
    "\n",
    "print(f\"Final graduation rate: {final_graduate_count}/{len(target_df)} ({final_rate:.1f}%)\")\n",
    "\n",
    "# Show final target distribution\n",
    "print(f\"\\nFinal Target Distribution:\")\n",
    "print(f\"  Graduated (1): {final_graduate_count} participants\")\n",
    "print(f\"  Not Graduated (0): {len(target_df) - final_graduate_count} participants\")\n",
    "\n",
    "# Class balance check\n",
    "if final_rate < 30 or final_rate > 70:\n",
    "    print(f\"‚ö†Ô∏è  Class imbalance detected! Consider SMOTE for modeling.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Reasonable class balance for modeling.\")\n",
    "\n",
    "# Merge final target back to feature matrix\n",
    "final_features_with_target = final_features.merge(\n",
    "    target_df[['id', 'graduated_final', 'completion_score']], \n",
    "    on='id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ FINAL DATASET READY!\")\n",
    "print(f\"Shape: {final_features_with_target.shape}\")\n",
    "print(f\"Features: {final_features_with_target.shape[1] - 1}\")  # -1 for target\n",
    "print(f\"Target column: 'graduated_final'\")\n",
    "\n",
    "# Save the final dataset\n",
    "# final_features_with_target.to_csv('bootcamp_final_dataset.csv', index=False)\n",
    "# print(f\"üíæ Dataset saved as 'bootcamp_final_dataset.csv'\")\n",
    "\n",
    "# Show sample of final dataset\n",
    "print(f\"\\nüìä Final Dataset Sample:\")\n",
    "sample_cols = ['id', 'attendance_rate', 'avg_pretest_score', 'total_submissions', 'graduated_final']\n",
    "print(final_features_with_target[sample_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d76cd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "feature_cols = [col for col in final_features_with_target.columns \n",
    "                if col not in ['id', 'graduated_final', 'completion_score', 'performance_level']]\n",
    "\n",
    "X = final_features_with_target[feature_cols]\n",
    "y = final_features_with_target['graduated_final']\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE untuk handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfc96f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features:\n",
      "                       feature  importance\n",
      "0      total_meetings_attended    0.341385\n",
      "5              attendance_rate    0.325185\n",
      "6             engagement_score    0.164568\n",
      "1              highest_meeting    0.119839\n",
      "4          avg_trainer_quality    0.014244\n",
      "3         avg_material_quality    0.009448\n",
      "7        pretest_ml_best_score    0.007874\n",
      "11  assessment_completion_rate    0.007718\n",
      "8        pretest_py_best_score    0.003823\n",
      "10           avg_pretest_score    0.003274\n"
     ]
    }
   ],
   "source": [
    "# Check which features are most important\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8155c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More conservative target (only if both methods agree)\n",
    "final_features_with_target['graduated_conservative'] = (\n",
    "    (target_df['graduated_rule'] == 1) & \n",
    "    (target_df['graduated_cluster'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# This would give ~43% graduation rate (more balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f2057a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ MODEL BUILDING & EVALUATION\n",
      "======================================================================\n",
      "\n",
      "üìä Step 1: Data Preparation\n",
      "----------------------------------------\n",
      "Available features: 18\n",
      "Features: ['total_meetings_attended', 'highest_meeting', 'first_meeting', 'avg_material_quality', 'avg_trainer_quality', 'attendance_rate', 'engagement_score', 'pretest_ml_best_score', 'pretest_py_best_score', 'pretest_st_best_score', 'avg_pretest_score', 'assessment_completion_rate', 'has_mini_project', 'has_weekly_quiz', 'total_submissions', 'status_encoded', 'batch', 'graduated_conservative']\n",
      "\n",
      "Missing values before imputation:\n",
      "925\n",
      "Missing values after imputation: 0\n",
      "\n",
      "Target distribution:\n",
      "  Not Graduated (0): 113 (20.6%)\n",
      "  Graduated (1): 436 (79.4%)\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ MODEL BUILDING & EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA PREPARATION\n",
    "# ============================================================================\n",
    "print(\"\\nüìä Step 1: Data Preparation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Prepare features (exclude non-predictive columns)\n",
    "exclude_cols = ['id', 'graduated_final', 'completion_score', 'performance_level']\n",
    "feature_cols = [col for col in final_features_with_target.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Available features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = final_features_with_target[feature_cols].copy()\n",
    "y = final_features_with_target['graduated_final'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values before imputation:\")\n",
    "print(X.isnull().sum().sum())\n",
    "\n",
    "# Simple imputation strategy\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in ['float64', 'int64']:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "    else:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'Unknown')\n",
    "\n",
    "print(f\"Missing values after imputation: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Check target distribution\n",
    "target_dist = y.value_counts().sort_index()\n",
    "print(f\"\\nTarget distribution:\")\n",
    "for label, count in target_dist.items():\n",
    "    pct = count / len(y) * 100\n",
    "    status = \"Graduated\" if label == 1 else \"Not Graduated\"\n",
    "    print(f\"  {status} ({label}): {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983594f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Step 2: Train-Test Split\n",
      "----------------------------------------\n",
      "Training set: (439, 18)\n",
      "Test set: (110, 18)\n",
      "\n",
      "Class distribution:\n",
      "  Train: Not Graduated=0.205, Graduated=0.795\n",
      "  Test:  Not Graduated=0.209, Graduated=0.791\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüîÑ Step 2: Train-Test Split\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "train_dist = y_train.value_counts(normalize=True).sort_index()\n",
    "test_dist = y_test.value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train: Not Graduated={train_dist[0]:.3f}, Graduated={train_dist[1]:.3f}\")\n",
    "print(f\"  Test:  Not Graduated={test_dist[0]:.3f}, Graduated={test_dist[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbd09993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Step 3: Model Definitions\n",
      "----------------------------------------\n",
      "Models to evaluate: ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'SVM']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. MODEL DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüß† Step 3: Model Definitions\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Models to evaluate: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "043d612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Step 4: Cross-Validation Evaluation\n",
      "----------------------------------------\n",
      "\n",
      "üîç Evaluating Logistic Regression...\n",
      "  Without SMOTE - Balanced Accuracy: 1.000 (+/- 0.000)\n",
      "  With SMOTE    - Balanced Accuracy: 1.000 (+/- 0.000)\n",
      "\n",
      "üîç Evaluating Random Forest...\n",
      "  Without SMOTE - Balanced Accuracy: 1.000 (+/- 0.000)\n",
      "  With SMOTE    - Balanced Accuracy: 1.000 (+/- 0.000)\n",
      "\n",
      "üîç Evaluating Gradient Boosting...\n",
      "  Without SMOTE - Balanced Accuracy: 1.000 (+/- 0.000)\n",
      "  With SMOTE    - Balanced Accuracy: 1.000 (+/- 0.000)\n",
      "\n",
      "üîç Evaluating SVM...\n",
      "  Without SMOTE - Balanced Accuracy: 0.872 (+/- 0.227)\n",
      "  With SMOTE    - Balanced Accuracy: 0.947 (+/- 0.015)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. CROSS-VALIDATION EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìà Step 4: Cross-Validation Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Store results\n",
    "cv_results = {}\n",
    "cv_scores_balanced = {}\n",
    "\n",
    "# Evaluate each model with and without SMOTE\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç Evaluating {name}...\")\n",
    "    \n",
    "    # Without SMOTE\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='balanced_accuracy')\n",
    "    cv_scores_balanced[f\"{name}\"] = scores\n",
    "    \n",
    "    print(f\"  Without SMOTE - Balanced Accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "    \n",
    "    # With SMOTE (using pipeline to avoid data leakage)\n",
    "    smote_pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    scores_smote = cross_val_score(smote_pipeline, X_train, y_train, cv=5, scoring='balanced_accuracy')\n",
    "    cv_scores_balanced[f\"{name} + SMOTE\"] = scores_smote\n",
    "    \n",
    "    print(f\"  With SMOTE    - Balanced Accuracy: {scores_smote.mean():.3f} (+/- {scores_smote.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d8ed849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Step 5: Best Model Selection\n",
      "----------------------------------------\n",
      "Best configuration: Logistic Regression\n",
      "Best CV balanced accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. BEST MODEL SELECTION & TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüèÜ Step 5: Best Model Selection\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find best performing combination\n",
    "best_score = 0\n",
    "best_config = None\n",
    "\n",
    "for config_name, scores in cv_scores_balanced.items():\n",
    "    mean_score = scores.mean()\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_config = config_name\n",
    "\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "print(f\"Best CV balanced accuracy: {best_score:.3f}\")\n",
    "\n",
    "# Train the best model\n",
    "use_smote = 'SMOTE' in best_config\n",
    "model_name = best_config.replace(' + SMOTE', '')\n",
    "best_model = models[model_name]\n",
    "\n",
    "if use_smote:\n",
    "    # Apply SMOTE to training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nSMOTE applied:\")\n",
    "    print(f\"  Before: {X_train.shape[0]} samples\")\n",
    "    print(f\"  After:  {X_train_resampled.shape[0]} samples\")\n",
    "    \n",
    "    # Train on resampled data\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "else:\n",
    "    # Train on original data\n",
    "    best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28450892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6: Test Set Evaluation\n",
      "----------------------------------------\n",
      "Test Set Performance:\n",
      "  Balanced Accuracy: 1.000\n",
      "  ROC AUC: 1.000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Graduated       1.00      1.00      1.00        23\n",
      "    Graduated       1.00      1.00      1.00        87\n",
      "\n",
      "     accuracy                           1.00       110\n",
      "    macro avg       1.00      1.00      1.00       110\n",
      " weighted avg       1.00      1.00      1.00       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "               0     1\n",
      "Actual   0     23     0\n",
      "         1      0    87\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. MODEL EVALUATION ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Step 6: Test Set Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  Balanced Accuracy: {balanced_acc:.3f}\")\n",
    "print(f\"  ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Graduated', 'Graduated']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"               0     1\")\n",
    "print(f\"Actual   0    {cm[0,0]:3d}   {cm[0,1]:3d}\")\n",
    "print(f\"         1    {cm[1,0]:3d}   {cm[1,1]:3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed238ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Step 7: Feature Importance Analysis\n",
      "----------------------------------------\n",
      "Top 10 Most Important Features (by coefficient magnitude):\n",
      "  total_meetings_attended  : 2.549\n",
      "  highest_meeting          : 2.549\n",
      "  status_encoded           : 0.274\n",
      "  pretest_ml_best_score    : 0.219\n",
      "  attendance_rate          : 0.087\n",
      "  avg_pretest_score        : 0.073\n",
      "  has_mini_project         : 0.066\n",
      "  total_submissions        : 0.062\n",
      "  engagement_score         : 0.053\n",
      "  batch                    : 0.051\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüéØ Step 7: Feature Importance Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']:<25}: {row['importance']:.3f}\")\n",
    "        \n",
    "    # Store for visualization\n",
    "    top_features = feature_importance.head(15)\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models, use coefficient magnitude\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': abs(best_model.coef_[0])\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features (by coefficient magnitude):\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']:<25}: {row['importance']:.3f}\")\n",
    "        \n",
    "    top_features = feature_importance.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d4d6445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Step 8: Model Insights & Recommendations\n",
      "----------------------------------------\n",
      "Model Performance Summary:\n",
      "  ‚úÖ Best Model: Logistic Regression\n",
      "  ‚úÖ Balanced Accuracy: 1.000\n",
      "  ‚úÖ ROC AUC: 1.000\n",
      "  üìà Performance Rating: Excellent\n",
      "\n",
      "üéØ Business Insights:\n",
      "  ‚Ä¢ Attendance rate is a key predictor of graduation\n",
      "\n",
      "üìã Recommendations for Model Deployment:\n",
      "  1. Use Logistic Regression for production predictions\n",
      "  2. Monitor key features: total_meetings_attended, highest_meeting, status_encoded\n",
      "  3. Set prediction threshold based on business needs\n",
      "  4. Regularly retrain model with new data\n",
      "\n",
      "‚úÖ Model building completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. MODEL INSIGHTS & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüí° Step 8: Model Insights & Recommendations\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"Model Performance Summary:\")\n",
    "print(f\"  ‚úÖ Best Model: {best_config}\")\n",
    "print(f\"  ‚úÖ Balanced Accuracy: {balanced_acc:.3f}\")\n",
    "print(f\"  ‚úÖ ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Performance interpretation\n",
    "if balanced_acc >= 0.8:\n",
    "    performance = \"Excellent\"\n",
    "elif balanced_acc >= 0.7:\n",
    "    performance = \"Good\"\n",
    "elif balanced_acc >= 0.6:\n",
    "    performance = \"Fair\"\n",
    "else:\n",
    "    performance = \"Poor\"\n",
    "\n",
    "print(f\"  üìà Performance Rating: {performance}\")\n",
    "\n",
    "# Business insights\n",
    "print(f\"\\nüéØ Business Insights:\")\n",
    "if 'attendance_rate' in feature_importance.head(5)['feature'].values:\n",
    "    print(f\"  ‚Ä¢ Attendance rate is a key predictor of graduation\")\n",
    "if 'avg_pretest_score' in feature_importance.head(5)['feature'].values:\n",
    "    print(f\"  ‚Ä¢ Academic performance (pretest scores) strongly indicates success\")\n",
    "if 'engagement_score' in feature_importance.head(5)['feature'].values:\n",
    "    print(f\"  ‚Ä¢ Student engagement level is crucial for completion\")\n",
    "\n",
    "print(f\"\\nüìã Recommendations for Model Deployment:\")\n",
    "print(f\"  1. Use {best_config} for production predictions\")\n",
    "print(f\"  2. Monitor key features: {', '.join(feature_importance.head(3)['feature'].values)}\")\n",
    "print(f\"  3. Set prediction threshold based on business needs\")\n",
    "print(f\"  4. Regularly retrain model with new data\")\n",
    "\n",
    "if use_smote:\n",
    "    print(f\"  5. Apply SMOTE for handling class imbalance in future training\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model building completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4e8e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Model results ready for saving...\n",
      "   Results stored in 'model_results' dictionary\n",
      "   Trained model available as 'best_model' object\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. SAVE MODEL RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Create results summary\n",
    "model_results = {\n",
    "    'best_model': best_config,\n",
    "    'balanced_accuracy': balanced_acc,\n",
    "    'roc_auc': roc_auc,\n",
    "    'feature_importance': feature_importance.to_dict('records'),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'use_smote': use_smote\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Model results ready for saving...\")\n",
    "print(f\"   Results stored in 'model_results' dictionary\")\n",
    "print(f\"   Trained model available as 'best_model' object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b2347fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ TEST DATA PROCESSING & PREDICTION\n",
      "======================================================================\n",
      "\n",
      "üìÇ Step 1: Loading Test Datasets\n",
      "----------------------------------------\n",
      "‚úÖ absensi: (760, 12)\n",
      "‚úÖ mini_project: (22, 5)\n",
      "‚úÖ pendaftaran: (21, 9)\n",
      "‚úÖ pretest_ml: (32, 14)\n",
      "‚úÖ pretest_py: (47, 14)\n",
      "‚úÖ pretest_st: (33, 19)\n",
      "‚úÖ weekly_quiz: (25, 5)\n",
      "\n",
      "üìã Unique participants per test dataset:\n",
      "  absensi: 48 unique IDs\n",
      "  mini_project: 22 unique IDs\n",
      "  pendaftaran: 21 unique IDs\n",
      "  pretest_ml: 27 unique IDs\n",
      "  pretest_py: 40 unique IDs\n",
      "  pretest_st: 30 unique IDs\n",
      "  weekly_quiz: 23 unique IDs\n",
      "\n",
      "üë• Total unique test participants: 48\n",
      "‚úÖ Test master table created with 48 participants\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÆ TEST DATA PROCESSING & PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD TEST DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìÇ Step 1: Loading Test Datasets\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test file mapping\n",
    "test_file_names = {\n",
    "    'absensi': 'MineToday Dataset/test/test_absensi.csv',\n",
    "    'mini_project': 'MineToday Dataset/test/test_mini_project.csv', \n",
    "    'pendaftaran': 'MineToday Dataset/test/test_pendaftaran.csv',\n",
    "    'pretest_ml': 'MineToday Dataset/test/test_pretest_ml.csv',\n",
    "    'pretest_py': 'MineToday Dataset/test/test_pretest_py.csv',\n",
    "    'pretest_st': 'MineToday Dataset/test/test_pretest_st.csv',\n",
    "    'weekly_quiz': 'MineToday Dataset/test/test_weekly_quiz.csv'\n",
    "}\n",
    "\n",
    "def load_test_datasets():\n",
    "    test_datasets = {}\n",
    "    for name, path in test_file_names.items():\n",
    "        try:\n",
    "            test_datasets[name] = pd.read_csv(path)\n",
    "            print(f\"‚úÖ {name}: {test_datasets[name].shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå {name}: File not found at {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {name}: Error loading - {e}\")\n",
    "    return test_datasets\n",
    "\n",
    "# Load test data\n",
    "test_datasets = load_test_datasets()\n",
    "\n",
    "# Get unique test participant IDs\n",
    "print(f\"\\nüìã Unique participants per test dataset:\")\n",
    "test_participant_ids = set()\n",
    "for name, df in test_datasets.items():\n",
    "    if 'id' in df.columns:\n",
    "        unique_ids = df['id'].nunique()\n",
    "        print(f\"  {name}: {unique_ids} unique IDs\")\n",
    "        test_participant_ids.update(df['id'].unique())\n",
    "\n",
    "print(f\"\\nüë• Total unique test participants: {len(test_participant_ids)}\")\n",
    "\n",
    "# Create master test participant table\n",
    "test_master_df = pd.DataFrame({'id': list(test_participant_ids)})\n",
    "print(f\"‚úÖ Test master table created with {len(test_master_df)} participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51e00f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è Step 2: Feature Engineering for Test Data\n",
      "----------------------------------------\n",
      "üìä Processing test attendance data...\n",
      "\n",
      "üìä Processing Attendance Data...\n",
      "  ‚úÖ Test attendance features: 48 participants\n",
      "üìä Processing test assessment data...\n",
      "\n",
      "üìä Processing Assessment Scores...\n",
      "  ‚úÖ Test assessment features: 42 participants\n",
      "üìä Processing test submission data...\n",
      "\n",
      "üìä Processing Submission Data...\n",
      "  ‚úÖ Test submission features: 26 participants\n",
      "üìä Processing test registration data...\n",
      "\n",
      "üìä Processing Registration Data...\n",
      "  ‚úÖ Test registration features: 21 participants\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# 2. FEATURE ENGINEERING FOR TEST DATA (REUSE FUNCTIONS)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è Step 2: Feature Engineering for Test Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Reuse the same feature engineering functions from training\n",
    "print(\"üìä Processing test attendance data...\")\n",
    "if 'absensi' in test_datasets:\n",
    "    test_attendance_features = create_attendance_features(test_datasets['absensi'])\n",
    "    print(f\"  ‚úÖ Test attendance features: {len(test_attendance_features)} participants\")\n",
    "\n",
    "print(\"üìä Processing test assessment data...\")\n",
    "test_assessment_features = create_assessment_features(test_datasets)\n",
    "print(f\"  ‚úÖ Test assessment features: {len(test_assessment_features)} participants\")\n",
    "\n",
    "print(\"üìä Processing test submission data...\")\n",
    "test_submission_features = create_submission_features(test_datasets)\n",
    "print(f\"  ‚úÖ Test submission features: {len(test_submission_features)} participants\")\n",
    "\n",
    "print(\"üìä Processing test registration data...\")\n",
    "if 'pendaftaran' in test_datasets:\n",
    "    test_registration_features = create_registration_features(test_datasets['pendaftaran'])\n",
    "    print(f\"  ‚úÖ Test registration features: {len(test_registration_features)} participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b5adda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Step 3: Merging Test Features\n",
      "----------------------------------------\n",
      "  ‚úÖ Merged test attendance: 48 ‚Üí 48 rows\n",
      "  ‚úÖ Merged test assessment: 48 ‚Üí 48 rows\n",
      "  ‚úÖ Merged test submission: 48 ‚Üí 48 rows\n",
      "  ‚úÖ Merged test registration: 48 ‚Üí 48 rows\n",
      "\n",
      "üéØ Final test feature matrix: (48, 19)\n",
      "Test columns: ['id', 'total_meetings_attended', 'highest_meeting', 'first_meeting', 'avg_material_quality', 'avg_trainer_quality', 'attendance_rate', 'engagement_score', 'pretest_ml_best_score', 'pretest_py_best_score', 'pretest_st_best_score', 'avg_pretest_score', 'assessment_completion_rate', 'performance_level', 'has_mini_project', 'has_weekly_quiz', 'total_submissions', 'status_encoded', 'batch']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. MERGE TEST FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîÑ Step 3: Merging Test Features\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Merge all test features\n",
    "final_test_features = test_master_df.copy()\n",
    "\n",
    "test_feature_sets = [\n",
    "    ('attendance', test_attendance_features),\n",
    "    ('assessment', test_assessment_features), \n",
    "    ('submission', test_submission_features),\n",
    "    ('registration', test_registration_features)\n",
    "]\n",
    "\n",
    "for name, features in test_feature_sets:\n",
    "    if features is not None and len(features) > 0:\n",
    "        before_count = len(final_test_features)\n",
    "        final_test_features = final_test_features.merge(features, on='id', how='left')\n",
    "        after_count = len(final_test_features)\n",
    "        print(f\"  ‚úÖ Merged test {name}: {before_count} ‚Üí {after_count} rows\")\n",
    "\n",
    "print(f\"\\nüéØ Final test feature matrix: {final_test_features.shape}\")\n",
    "print(f\"Test columns: {list(final_test_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af51e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ TEST DATA PROCESSING & PREDICTION\n",
      "======================================================================\n",
      "\n",
      "üìÇ Step 1: Loading Test Datasets\n",
      "----------------------------------------\n",
      "‚úÖ absensi: (760, 12)\n",
      "‚úÖ mini_project: (22, 5)\n",
      "‚úÖ pendaftaran: (21, 9)\n",
      "‚úÖ pretest_ml: (32, 14)\n",
      "‚úÖ pretest_py: (47, 14)\n",
      "‚úÖ pretest_st: (33, 19)\n",
      "‚úÖ weekly_quiz: (25, 5)\n",
      "\n",
      "üìã Unique participants per test dataset:\n",
      "  absensi: 48 unique IDs\n",
      "  mini_project: 22 unique IDs\n",
      "  pendaftaran: 21 unique IDs\n",
      "  pretest_ml: 27 unique IDs\n",
      "  pretest_py: 40 unique IDs\n",
      "  pretest_st: 30 unique IDs\n",
      "  weekly_quiz: 23 unique IDs\n",
      "\n",
      "üë• Total unique test participants: 48\n",
      "‚úÖ Test master table created with 48 participants\n",
      "\n",
      "üõ†Ô∏è Step 2: Feature Engineering for Test Data\n",
      "----------------------------------------\n",
      "üìä Processing test attendance data...\n",
      "\n",
      "üìä Processing Attendance Data...\n",
      "  ‚úÖ Test attendance features: 48 participants\n",
      "üìä Processing test assessment data...\n",
      "\n",
      "üìä Processing Assessment Scores...\n",
      "  ‚úÖ Test assessment features: 42 participants\n",
      "üìä Processing test submission data...\n",
      "\n",
      "üìä Processing Submission Data...\n",
      "  ‚úÖ Test submission features: 26 participants\n",
      "üìä Processing test registration data...\n",
      "\n",
      "üìä Processing Registration Data...\n",
      "  ‚úÖ Test registration features: 21 participants\n",
      "\n",
      "üîÑ Step 3: Merging Test Features\n",
      "----------------------------------------\n",
      "  ‚úÖ Merged test attendance: 48 ‚Üí 48 rows\n",
      "  ‚úÖ Merged test assessment: 48 ‚Üí 48 rows\n",
      "  ‚úÖ Merged test submission: 48 ‚Üí 48 rows\n",
      "  ‚úÖ Merged test registration: 48 ‚Üí 48 rows\n",
      "\n",
      "üéØ Final test feature matrix: (48, 19)\n",
      "Test columns: ['id', 'total_meetings_attended', 'highest_meeting', 'first_meeting', 'avg_material_quality', 'avg_trainer_quality', 'attendance_rate', 'engagement_score', 'pretest_ml_best_score', 'pretest_py_best_score', 'pretest_st_best_score', 'avg_pretest_score', 'assessment_completion_rate', 'performance_level', 'has_mini_project', 'has_weekly_quiz', 'total_submissions', 'status_encoded', 'batch']\n",
      "\n",
      "üîß Step 4: Prepare Test Data for Prediction\n",
      "----------------------------------------\n",
      "Expected feature columns: 18\n",
      "Available features in test: 17\n",
      "Missing features in test: ['graduated_conservative']\n",
      "  ‚úÖ Filled missing feature 'graduated_conservative' with default value: 0\n",
      "\n",
      "Handling missing values in test data...\n",
      "Missing values before imputation: 244\n",
      "Missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîÆ TEST DATA PROCESSING & PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD TEST DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìÇ Step 1: Loading Test Datasets\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test file mapping\n",
    "test_file_names = {\n",
    "    'absensi': 'MineToday Dataset/test/test_absensi.csv',\n",
    "    'mini_project': 'MineToday Dataset/test/test_mini_project.csv', \n",
    "    'pendaftaran': 'MineToday Dataset/test/test_pendaftaran.csv',\n",
    "    'pretest_ml': 'MineToday Dataset/test/test_pretest_ml.csv',\n",
    "    'pretest_py': 'MineToday Dataset/test/test_pretest_py.csv',\n",
    "    'pretest_st': 'MineToday Dataset/test/test_pretest_st.csv',\n",
    "    'weekly_quiz': 'MineToday Dataset/test/test_weekly_quiz.csv'\n",
    "}\n",
    "\n",
    "def load_test_datasets():\n",
    "    test_datasets = {}\n",
    "    for name, path in test_file_names.items():\n",
    "        try:\n",
    "            test_datasets[name] = pd.read_csv(path)\n",
    "            print(f\"‚úÖ {name}: {test_datasets[name].shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå {name}: File not found at {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {name}: Error loading - {e}\")\n",
    "    return test_datasets\n",
    "\n",
    "# Load test data\n",
    "test_datasets = load_test_datasets()\n",
    "\n",
    "# Get unique test participant IDs\n",
    "print(f\"\\nüìã Unique participants per test dataset:\")\n",
    "test_participant_ids = set()\n",
    "for name, df in test_datasets.items():\n",
    "    if 'id' in df.columns:\n",
    "        unique_ids = df['id'].nunique()\n",
    "        print(f\"  {name}: {unique_ids} unique IDs\")\n",
    "        test_participant_ids.update(df['id'].unique())\n",
    "\n",
    "print(f\"\\nüë• Total unique test participants: {len(test_participant_ids)}\")\n",
    "\n",
    "# Create master test participant table\n",
    "test_master_df = pd.DataFrame({'id': list(test_participant_ids)})\n",
    "print(f\"‚úÖ Test master table created with {len(test_master_df)} participants\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. FEATURE ENGINEERING FOR TEST DATA (REUSE FUNCTIONS)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è Step 2: Feature Engineering for Test Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Reuse the same feature engineering functions from training\n",
    "print(\"üìä Processing test attendance data...\")\n",
    "if 'absensi' in test_datasets:\n",
    "    test_attendance_features = create_attendance_features(test_datasets['absensi'])\n",
    "    print(f\"  ‚úÖ Test attendance features: {len(test_attendance_features)} participants\")\n",
    "\n",
    "print(\"üìä Processing test assessment data...\")\n",
    "test_assessment_features = create_assessment_features(test_datasets)\n",
    "print(f\"  ‚úÖ Test assessment features: {len(test_assessment_features)} participants\")\n",
    "\n",
    "print(\"üìä Processing test submission data...\")\n",
    "test_submission_features = create_submission_features(test_datasets)\n",
    "print(f\"  ‚úÖ Test submission features: {len(test_submission_features)} participants\")\n",
    "\n",
    "print(\"üìä Processing test registration data...\")\n",
    "if 'pendaftaran' in test_datasets:\n",
    "    test_registration_features = create_registration_features(test_datasets['pendaftaran'])\n",
    "    print(f\"  ‚úÖ Test registration features: {len(test_registration_features)} participants\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MERGE TEST FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîÑ Step 3: Merging Test Features\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Merge all test features\n",
    "final_test_features = test_master_df.copy()\n",
    "\n",
    "test_feature_sets = [\n",
    "    ('attendance', test_attendance_features),\n",
    "    ('assessment', test_assessment_features), \n",
    "    ('submission', test_submission_features),\n",
    "    ('registration', test_registration_features)\n",
    "]\n",
    "\n",
    "for name, features in test_feature_sets:\n",
    "    if features is not None and len(features) > 0:\n",
    "        before_count = len(final_test_features)\n",
    "        final_test_features = final_test_features.merge(features, on='id', how='left')\n",
    "        after_count = len(final_test_features)\n",
    "        print(f\"  ‚úÖ Merged test {name}: {before_count} ‚Üí {after_count} rows\")\n",
    "\n",
    "print(f\"\\nüéØ Final test feature matrix: {final_test_features.shape}\")\n",
    "print(f\"Test columns: {list(final_test_features.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PREPARE TEST DATA FOR PREDICTION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîß Step 4: Prepare Test Data for Prediction\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Use the same feature columns as training\n",
    "# (Exclude target and non-predictive columns)\n",
    "exclude_cols = ['id', 'graduated_final', 'completion_score', 'performance_level']\n",
    "feature_cols_for_prediction = [col for col in final_features_with_target.columns \n",
    "                               if col not in exclude_cols]\n",
    "\n",
    "print(f\"Expected feature columns: {len(feature_cols_for_prediction)}\")\n",
    "\n",
    "# Select only the feature columns that exist in test data\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for col in feature_cols_for_prediction:\n",
    "    if col in final_test_features.columns:\n",
    "        available_features.append(col)\n",
    "    else:\n",
    "        missing_features.append(col)\n",
    "\n",
    "print(f\"Available features in test: {len(available_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing features in test: {missing_features}\")\n",
    "    # Fill missing features with default values\n",
    "    for col in missing_features:\n",
    "        if col in final_features_with_target.columns:\n",
    "            # Use median/mode from training data\n",
    "            if final_features_with_target[col].dtype in ['float64', 'int64']:\n",
    "                default_value = final_features_with_target[col].median()\n",
    "            else:\n",
    "                default_value = final_features_with_target[col].mode()[0] if not final_features_with_target[col].mode().empty else 'Unknown'\n",
    "            final_test_features[col] = default_value\n",
    "            available_features.append(col)\n",
    "            print(f\"  ‚úÖ Filled missing feature '{col}' with default value: {default_value}\")\n",
    "\n",
    "# Prepare test feature matrix\n",
    "X_test_final = final_test_features[available_features].copy()\n",
    "\n",
    "# Handle missing values in test data (same strategy as training)\n",
    "print(f\"\\nHandling missing values in test data...\")\n",
    "print(f\"Missing values before imputation: {X_test_final.isnull().sum().sum()}\")\n",
    "\n",
    "for col in X_test_final.columns:\n",
    "    if X_test_final[col].dtype in ['float64', 'int64']:\n",
    "        # Use training data statistics for imputation\n",
    "        if col in final_features_with_target.columns:\n",
    "            fill_value = final_features_with_target[col].median()\n",
    "        else:\n",
    "            fill_value = X_test_final[col].median()\n",
    "        X_test_final[col] = X_test_final[col].fillna(fill_value)\n",
    "    else:\n",
    "        if col in final_features_with_target.columns:\n",
    "            fill_value = final_features_with_target[col].mode()[0] if not final_features_with_target[col].mode().empty else 'Unknown'\n",
    "        else:\n",
    "            fill_value = X_test_final[col].mode()[0] if not X_test_final[col].mode().empty else 'Unknown'\n",
    "        X_test_final[col] = X_test_final[col].fillna(fill_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {X_test_final.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1fd1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Step 5: Generate Predictions\n",
      "----------------------------------------\n",
      "Generating predictions with trained model...\n",
      "‚úÖ Predictions generated for 48 test participants\n",
      "\n",
      "Test Prediction Distribution:\n",
      "  Not Graduated (0): 33 (68.8%)\n",
      "  Graduated (1): 15 (31.2%)\n",
      "\n",
      "Prediction Statistics:\n",
      "  Mean probability: 0.314\n",
      "  Std probability:  0.463\n",
      "  Min probability:  0.000\n",
      "  Max probability:  1.000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GENERATE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîÆ Step 5: Generate Predictions\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Make predictions using the best trained model\n",
    "print(\"Generating predictions with trained model...\")\n",
    "\n",
    "try:\n",
    "    # Predict probabilities and binary labels\n",
    "    test_predictions_proba = best_model.predict_proba(X_test_final)[:, 1]\n",
    "    test_predictions_binary = best_model.predict(X_test_final)\n",
    "    \n",
    "    print(f\"‚úÖ Predictions generated for {len(test_predictions_binary)} test participants\")\n",
    "    \n",
    "    # Create results dataframe\n",
    "    test_results = pd.DataFrame({\n",
    "        'id': final_test_features['id'],\n",
    "        'graduated_probability': test_predictions_proba,\n",
    "        'graduated_prediction': test_predictions_binary\n",
    "    })\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    pred_dist = pd.Series(test_predictions_binary).value_counts().sort_index()\n",
    "    print(f\"\\nTest Prediction Distribution:\")\n",
    "    for label, count in pred_dist.items():\n",
    "        pct = count / len(test_predictions_binary) * 100\n",
    "        status = \"Graduated\" if label == 1 else \"Not Graduated\"\n",
    "        print(f\"  {status} ({label}): {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"  Mean probability: {test_predictions_proba.mean():.3f}\")\n",
    "    print(f\"  Std probability:  {test_predictions_proba.std():.3f}\")\n",
    "    print(f\"  Min probability:  {test_predictions_proba.min():.3f}\")\n",
    "    print(f\"  Max probability:  {test_predictions_proba.max():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating predictions: {e}\")\n",
    "    print(\"Make sure the trained model 'best_model' is available from previous steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b8502e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Step 6: Prepare Submission File\n",
      "----------------------------------------\n",
      "Test results shape: (48, 3)\n",
      "Test results columns: ['id', 'graduated_probability', 'graduated_prediction']\n",
      "Test results sample:\n",
      "                                     id  graduated_probability  \\\n",
      "0  2e73eb02-d3c9-4507-bd18-74911ed44215           2.280991e-17   \n",
      "1  1c33125d-19b3-44c5-b2f2-e8dcd0863df5           2.155581e-17   \n",
      "2  04eacef3-adce-4ab6-b5ea-4cbb4499d86a           2.265716e-17   \n",
      "\n",
      "   graduated_prediction  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "‚úÖ Loaded prediction_id.csv: (48, 1)\n",
      "Columns: ['ID']\n",
      "Sample data:\n",
      "                                     ID\n",
      "0  0028102b-576f-4819-b1df-8c0e7ae0247b\n",
      "1  008479a4-622a-4a22-8e5a-81e671535445\n",
      "2  009762eb-c062-41fd-9a2b-144b65f33c3b\n",
      "Renamed 'ID' column to 'id'\n",
      "\n",
      "Before merge:\n",
      "  prediction_ids IDs: 48\n",
      "  test_results IDs: 48\n",
      "  Common IDs: 48\n",
      "‚úÖ Merge completed: (48, 2)\n",
      "\n",
      "üìä Final Submission Shape: (48, 2)\n",
      "Submission columns: ['ID', 'label']\n",
      "\n",
      "Submission sample:\n",
      "                                     ID        label\n",
      "0  0028102b-576f-4819-b1df-8c0e7ae0247b        Lulus\n",
      "1  008479a4-622a-4a22-8e5a-81e671535445  Tidak Lulus\n",
      "2  009762eb-c062-41fd-9a2b-144b65f33c3b  Tidak Lulus\n",
      "3  014747f3-0710-4304-bf06-77433ef94d09        Lulus\n",
      "4  0289a1dd-32ab-46a3-bd41-e8741e2728fb  Tidak Lulus\n",
      "5  02b0def5-66b4-48e5-83ab-5905fee6b9db        Lulus\n",
      "6  049bb825-7731-47ae-8669-fd9c1d5e44ea        Lulus\n",
      "7  04eacef3-adce-4ab6-b5ea-4cbb4499d86a  Tidak Lulus\n",
      "8  051e0e70-94cf-4eea-b51a-81c5e3896ef4        Lulus\n",
      "9  052163ac-09b6-42b7-9543-63a104b92f5a  Tidak Lulus\n",
      "‚úÖ Submission format validated: ID,label with Lulus/Tidak Lulus\n",
      "\n",
      "üíæ Submission file saved as: bootcamp_graduation_predictions.csv\n",
      "\n",
      "Final Submission Distribution:\n",
      "  Tidak Lulus: 33 (68.8%)\n",
      "  Lulus: 15 (31.2%)\n",
      "\n",
      "‚úÖ Test data processing and prediction generation completed!\n",
      "üìã Summary:\n",
      "   ‚Ä¢ Processed 48 test participants\n",
      "   ‚Ä¢ Generated predictions using Logistic Regression\n",
      "   ‚Ä¢ Created submission file: bootcamp_graduation_predictions.csv\n",
      "   ‚Ä¢ Ready for Kaggle submission! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. PREPARE SUBMISSION FILE\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìÑ Step 6: Prepare Submission File\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Debug: Check if test_results exists and its structure\n",
    "try:\n",
    "    print(f\"Test results shape: {test_results.shape}\")\n",
    "    print(f\"Test results columns: {list(test_results.columns)}\")\n",
    "    print(f\"Test results sample:\")\n",
    "    print(test_results.head(3))\n",
    "except NameError:\n",
    "    print(\"‚ùå test_results not defined. Creating it now...\")\n",
    "    test_results = pd.DataFrame({\n",
    "        'id': final_test_features['id'],\n",
    "        'graduated_probability': test_predictions_proba,\n",
    "        'graduated_prediction': test_predictions_binary\n",
    "    })\n",
    "    print(f\"‚úÖ Created test_results: {test_results.shape}\")\n",
    "\n",
    "# Load prediction_id.csv to get required format\n",
    "try:\n",
    "    prediction_ids = pd.read_csv('MineToday Dataset/prediction_id.csv')\n",
    "    print(f\"‚úÖ Loaded prediction_id.csv: {prediction_ids.shape}\")\n",
    "    print(f\"Columns: {list(prediction_ids.columns)}\")\n",
    "    print(f\"Sample data:\")\n",
    "    print(prediction_ids.head(3))\n",
    "    \n",
    "    # Check if 'id' column exists in prediction_ids (handle both cases)\n",
    "    id_col = None\n",
    "    if 'id' in prediction_ids.columns:\n",
    "        id_col = 'id'\n",
    "    elif 'ID' in prediction_ids.columns:\n",
    "        id_col = 'ID'\n",
    "        prediction_ids = prediction_ids.rename(columns={'ID': 'id'})\n",
    "        print(f\"Renamed 'ID' column to 'id'\")\n",
    "    elif 'Id' in prediction_ids.columns:\n",
    "        id_col = 'Id'\n",
    "        prediction_ids = prediction_ids.rename(columns={'Id': 'id'})\n",
    "        print(f\"Renamed 'Id' column to 'id'\")\n",
    "    \n",
    "    if id_col is None:\n",
    "        # Try to find ID column with different name\n",
    "        possible_id_cols = [col for col in prediction_ids.columns if 'id' in col.lower()]\n",
    "        if possible_id_cols:\n",
    "            id_col = possible_id_cols[0]\n",
    "            print(f\"Found ID column: '{id_col}', renaming to 'id'\")\n",
    "            prediction_ids = prediction_ids.rename(columns={id_col: 'id'})\n",
    "        else:\n",
    "            print(f\"‚ùå No ID column found in prediction_id.csv\")\n",
    "            print(f\"Available columns: {list(prediction_ids.columns)}\")\n",
    "            raise KeyError(\"No ID column found\")\n",
    "    \n",
    "    # Debug: Check test_results structure before merge\n",
    "    print(f\"\\nBefore merge:\")\n",
    "    print(f\"  prediction_ids IDs: {prediction_ids['id'].nunique()}\")\n",
    "    print(f\"  test_results IDs: {test_results['id'].nunique()}\")\n",
    "    \n",
    "    # Check for overlap\n",
    "    common_ids = set(prediction_ids['id']).intersection(set(test_results['id']))\n",
    "    print(f\"  Common IDs: {len(common_ids)}\")\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(\"‚ö†Ô∏è  Warning: No common IDs found between prediction_id.csv and test data\")\n",
    "        print(\"Sample prediction_ids IDs:\", prediction_ids['id'].head(3).tolist())\n",
    "        print(\"Sample test_results IDs:\", test_results['id'].head(3).tolist())\n",
    "    \n",
    "    # Merge with predictions\n",
    "    submission_df = prediction_ids.merge(\n",
    "        test_results[['id', 'graduated_prediction']], \n",
    "        on='id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Merge completed: {submission_df.shape}\")\n",
    "    \n",
    "    # Check for missing predictions\n",
    "    missing_predictions = submission_df['graduated_prediction'].isnull().sum()\n",
    "    if missing_predictions > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {missing_predictions} participants in prediction_id.csv not found in test data\")\n",
    "        \n",
    "        # Fill with default prediction (majority class from training)\n",
    "        try:\n",
    "            majority_class = final_features_with_target['graduated_final'].mode()[0]\n",
    "        except (NameError, KeyError):\n",
    "            # Fallback to most common prediction in test results\n",
    "            majority_class = test_results['graduated_prediction'].mode()[0] if len(test_results) > 0 else 1\n",
    "            \n",
    "        submission_df['graduated_prediction'] = submission_df['graduated_prediction'].fillna(majority_class)\n",
    "        print(f\"   Filled missing predictions with majority class: {majority_class} ({'Lulus' if majority_class == 1 else 'Tidak Lulus'})\")\n",
    "    \n",
    "    # Rename column to match submission format\n",
    "    if 'graduated_prediction' in submission_df.columns:\n",
    "        # Convert 0/1 to \"Tidak Lulus\"/\"Lulus\"\n",
    "        submission_df['label'] = submission_df['graduated_prediction'].map({\n",
    "            0: 'Tidak Lulus',\n",
    "            1: 'Lulus'\n",
    "        })\n",
    "        # Drop the old column\n",
    "        submission_df = submission_df.drop('graduated_prediction', axis=1)\n",
    "    \n",
    "    # Ensure ID column is uppercase (as per submission format)\n",
    "    if 'id' in submission_df.columns:\n",
    "        submission_df = submission_df.rename(columns={'id': 'ID'})\n",
    "    \n",
    "    print(f\"\\nüìä Final Submission Shape: {submission_df.shape}\")\n",
    "    print(f\"Submission columns: {list(submission_df.columns)}\")\n",
    "    print(f\"\\nSubmission sample:\")\n",
    "    print(submission_df.head(10))\n",
    "    \n",
    "    # Validate submission format\n",
    "    if 'ID' in submission_df.columns and 'label' in submission_df.columns:\n",
    "        print(\"‚úÖ Submission format validated: ID,label with Lulus/Tidak Lulus\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Warning: Submission format may not match expected format\")\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_filename = 'bootcamp_graduation_predictions.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"\\nüíæ Submission file saved as: {submission_filename}\")\n",
    "    \n",
    "    # Final validation\n",
    "    if 'label' in submission_df.columns:\n",
    "        final_dist = submission_df['label'].value_counts()\n",
    "        print(f\"\\nFinal Submission Distribution:\")\n",
    "        for label, count in final_dist.items():\n",
    "            pct = count / len(submission_df) * 100\n",
    "            print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå prediction_id.csv not found. Creating submission with all test participants...\")\n",
    "    \n",
    "    # Create submission with all test results\n",
    "    submission_df = test_results[['id', 'graduated_prediction']].copy()\n",
    "    \n",
    "    # Convert to proper format\n",
    "    submission_df['label'] = submission_df['graduated_prediction'].map({\n",
    "        0: 'Tidak Lulus',\n",
    "        1: 'Lulus'\n",
    "    })\n",
    "    submission_df = submission_df.rename(columns={'id': 'ID'})\n",
    "    submission_df = submission_df[['ID', 'label']]  # Keep only required columns\n",
    "    \n",
    "    submission_filename = 'bootcamp_graduation_predictions.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"üíæ Submission file saved as: {submission_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in submission preparation: {e}\")\n",
    "    print(\"Creating basic submission file...\")\n",
    "    \n",
    "    # Fallback: Create basic submission\n",
    "    try:\n",
    "        basic_submission = pd.DataFrame({\n",
    "            'ID': final_test_features['id'],\n",
    "            'label': pd.Series(test_predictions_binary).map({\n",
    "                0: 'Tidak Lulus',\n",
    "                1: 'Lulus'\n",
    "            })\n",
    "        })\n",
    "        \n",
    "        submission_filename = 'bootcamp_graduation_predictions.csv'\n",
    "        basic_submission.to_csv(submission_filename, index=False)\n",
    "        print(f\"üíæ Basic submission file saved as: {submission_filename}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Failed to create basic submission: {e2}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Test data processing and prediction generation completed!\")\n",
    "\n",
    "# Debug final summary\n",
    "try:\n",
    "    print(f\"üìã Summary:\")\n",
    "    print(f\"   ‚Ä¢ Processed {len(test_participant_ids)} test participants\")\n",
    "    print(f\"   ‚Ä¢ Generated predictions using {best_config}\")\n",
    "    print(f\"   ‚Ä¢ Created submission file: {submission_filename}\")\n",
    "    print(f\"   ‚Ä¢ Ready for Kaggle submission! üöÄ\")\n",
    "except NameError as e:\n",
    "    print(f\"üìã Summary (some variables may not be defined): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c53be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
